{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1746023934651300"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "torch.seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Network for the Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Embedding with Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingLayer(torch.nn.Module):\n",
    "    def __init__(self, vocab_size: int, d_model: int):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.embedding = torch.nn.Embedding(self.vocab_size, self.d_model)\n",
    "        \n",
    "    def positional_encoding(self, num_words):\n",
    "        position_vector = torch.zeros(num_words, self.d_model)\n",
    "        \n",
    "        for pos in range(num_words):\n",
    "            i = torch.arange(self.d_model)\n",
    "            \n",
    "            i_even = i[0::2]\n",
    "            i_odd = i[1::2]\n",
    "            \n",
    "            position_vector[pos, 0::2] = torch.sin(pos/(10000**(2*i_even/self.d_model)))\n",
    "            position_vector[pos, 1::2] = torch.cos(pos/(10000**(2*i_odd/self.d_model)))\n",
    "        \n",
    "        return position_vector\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if len(x.shape) == 1:\n",
    "            x = torch.unsqueeze(x, 0) # adding batch of 1\n",
    "            \n",
    "        batch_size = x.size(0)\n",
    "        seq_num = x.size(1)\n",
    "        output = torch.zeros(batch_size, seq_num, self.d_model)\n",
    "        for batch in range(batch_size):\n",
    "            output[batch,:,:] = self.embedding(x[batch,:]) + self.positional_encoding(seq_num)\n",
    "        # x = self.positional_encoding(seq_num)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self Attention Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(torch.nn.Module):\n",
    "    def __init__(self, d_model: int, d_h: int, mask: bool = False):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_h = d_h\n",
    "        self.mask = mask\n",
    "        \n",
    "        self.WK = torch.nn.Linear(self.d_model, self.d_h, bias=False)\n",
    "        self.WQ = torch.nn.Linear(self.d_model, self.d_h, bias=False)\n",
    "        self.WV = torch.nn.Linear(self.d_model, self.d_h, bias=False)\n",
    "        self.softmax = torch.nn.Softmax(dim=1) # assumes batch first\n",
    "        \n",
    "        \n",
    "    def forward(self, key, query, value):\n",
    "        key = self.WK(key)\n",
    "        query = self.WQ(query)\n",
    "        value = self.WV(value)\n",
    "        if self.mask:\n",
    "            q_k = query @ torch.transpose(key, 1, 2)\n",
    "            # create the mask\n",
    "            mask = torch.ones(q_k.size(1), q_k.size(1)) # assumes batch first\n",
    "            rows, cols = torch.triu_indices(mask.size(0), mask.size(1), offset=1)\n",
    "            mask[rows, cols] = -1*torch.inf\n",
    "            x = self.softmax((q_k + mask)/math.sqrt(float(self.d_model)))\n",
    "        else:\n",
    "            x = self.softmax((query @ torch.transpose(key, 1, 2))/math.sqrt(float(self.d_model)))\n",
    "        \n",
    "        print(x.size(), value.size())\n",
    "        x = x @ value\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, num_heads: int, d_model: int, mask : bool = False):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        d_h = self.d_model // self.num_heads\n",
    "        \n",
    "        self.self_attention_blocks = torch.nn.ModuleList(\n",
    "            [SelfAttention(\n",
    "                self.d_model, d_h, mask\n",
    "            ) for _ in range(self.num_heads)]\n",
    "        )\n",
    "        \n",
    "        self.WO = torch.nn.Linear(self.d_model, self.d_model, bias=False)\n",
    "        \n",
    "    def forward(self, key, query, value):\n",
    "        x = [\n",
    "                attention(key, query, value) for attention in self.self_attention_blocks\n",
    "            ]\n",
    "        x = torch.concat((x), 2)\n",
    "        x = self.WO(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedforward Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ad-hoc model for attention is all you need architecture\n",
    "class Feedforward(torch.nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.layer1 = torch.nn.Linear(self.d_model, 2048)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.layer2 = torch.nn.Linear(2048, self.d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer2(self.relu(self.layer1(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, d_model, num_heads, mask: bool = False):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.mask = mask\n",
    "        # self.num_blocks = num_blocks\n",
    "        \n",
    "        self.multihead = MultiHeadAttention(\n",
    "            num_heads= self.num_heads,\n",
    "            d_model = self.d_model,\n",
    "            mask=self.mask\n",
    "        )\n",
    "        self.layer_norm = torch.nn.LayerNorm(self.d_model)\n",
    "        \n",
    "        self.feedforward = Feedforward(self.d_model)\n",
    "        \n",
    "    def forward(self, key, query, value):\n",
    "        x_multihead = self.multihead(\n",
    "            key, query, value\n",
    "        )\n",
    "        x_norm = self.layer_norm(key + x_multihead)\n",
    "        \n",
    "        x_feedforward = self.layer_norm(x_norm)\n",
    "        \n",
    "        x = self.layer_norm(x_feedforward + x_norm)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        self.multihead_mask = MultiHeadAttention(\n",
    "            num_heads= self.num_heads,\n",
    "            d_model = self.d_model,\n",
    "            mask=True\n",
    "        )\n",
    "        self.multihead_cross = MultiHeadAttention(\n",
    "            num_heads=self.num_heads,\n",
    "            d_model = self.d_model,\n",
    "            mask=False\n",
    "        )\n",
    "        self.layer_norm = torch.nn.LayerNorm(self.d_model)\n",
    "        self.feedforward = Feedforward(self.d_model)\n",
    "        \n",
    "    def forward(self,\n",
    "                key, query, value,\n",
    "                key_enc, value_enc):\n",
    "        # first stage\n",
    "        x_masked_multihead = self.multihead_mask(\n",
    "            key, query, value\n",
    "        )\n",
    "        x_norm1 = self.layer_norm(key + x_masked_multihead)\n",
    "        \n",
    "        # second stage cross attention\n",
    "        x_cross_attention = self.multihead_cross(\n",
    "            key_enc, x_norm1, value_enc\n",
    "        )\n",
    "\n",
    "        x_norm2 = self.layer_norm(x_norm1 + x_cross_attention)\n",
    "        \n",
    "        # feedforward\n",
    "        x_feedforward = self.feedforward(x_norm2)\n",
    "        \n",
    "        x = self.layer_norm(x_norm2 + x_feedforward)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutputBlock(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.layer1 = torch.nn.Linear(self.d_model, vocab_size)\n",
    "        self.softmax = torch.nn.Softmax(dim=2) # assumes batch first\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = self.softmax(self.layer1(x))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(torch.nn.Module):\n",
    "    def __init__(self, num_blocks: int, d_model: int,\n",
    "                num_heads: int, vocab_size_input: int,\n",
    "                vocab_size_output: int):\n",
    "        super().__init__()\n",
    "        self.num_blocks = num_blocks\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size_input = vocab_size_input\n",
    "        self.vocab_size_output = vocab_size_output\n",
    "        \n",
    "        self.input_embedding = EmbeddingLayer(\n",
    "            vocab_size=self.vocab_size_input,\n",
    "            d_model=self.d_model\n",
    "        )\n",
    "        \n",
    "        self.output_embedding = EmbeddingLayer(\n",
    "            vocab_size=self.vocab_size_output,\n",
    "            d_model=self.d_model\n",
    "        )\n",
    "        \n",
    "        self.encoder_blocks = torch.nn.ModuleList([\n",
    "            Encoder(\n",
    "                d_model=self.d_model,\n",
    "                num_heads=self.num_heads,\n",
    "                mask=False\n",
    "            ) for _ in range(self.num_blocks)\n",
    "        ])\n",
    "        \n",
    "        self.decoder_blocks = torch.nn.ModuleList([\n",
    "            Decoder(\n",
    "                d_model=self.d_model,\n",
    "                num_heads=self.num_heads\n",
    "            )\n",
    "        ])\n",
    "        \n",
    "        self.output = OutputBlock(\n",
    "            d_model=self.d_model,\n",
    "            vocab_size=self.vocab_size_output\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x_tokens, y_tokens):\n",
    "        x = self.input_embedding(x_tokens)\n",
    "        y = self.output_embedding(y_tokens)\n",
    "        \n",
    "        # encoder\n",
    "        for i in range(self.num_blocks):\n",
    "            x = self.encoder_blocks[i](x,x,x)\n",
    "            \n",
    "        x_enc = torch.clone(x)\n",
    "        \n",
    "        # decoder\n",
    "        for i in range(self.num_blocks):\n",
    "            y = self.decoder_blocks[i](\n",
    "                key=y, \n",
    "                query=y,\n",
    "                value=y,\n",
    "                key_enc = x_enc,\n",
    "                value_enc=x_enc\n",
    "            )\n",
    "        \n",
    "        # output layer\n",
    "        output = self.output(y)\n",
    "        \n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_embedding = EmbeddingLayer(\n",
    "    vocab_size=30000,\n",
    "    d_model=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = torch.tensor([\n",
    "    [2,1,3,0],\n",
    "    [1,2,3,4]\n",
    "])\n",
    "\n",
    "values_output = torch.tensor(\n",
    "    [\n",
    "        [1,2,0],\n",
    "        [1,0,0]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = torch.tensor([[2,1,3]])\n",
    "# outputs = torch.tensor([[2,1,4,3]])\n",
    "inputs_embeddings = input_embedding(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1569,  0.6992,  0.2809,  ...,  1.6644,  0.0277, -0.1062],\n",
       "         [-0.3128,  0.5446, -0.4950,  ..., -0.1689,  0.3440,  0.3188],\n",
       "         [ 0.3052,  0.2376,  2.9668,  ...,  0.8398, -0.3198,  0.9821],\n",
       "         [-1.3310, -1.1122,  0.4555,  ..., -1.0619,  0.7376,  0.0490]],\n",
       "\n",
       "        [[-1.1543,  0.9749, -1.2970,  ..., -0.1689,  0.3440,  0.3188],\n",
       "         [ 0.6846,  0.2689,  1.0829,  ...,  1.6644,  0.0277, -0.1062],\n",
       "         [ 0.3052,  0.2376,  2.9668,  ...,  0.8398, -0.3198,  0.9821],\n",
       "         [ 0.1949, -1.3591, -0.1936,  ..., -1.1712, -1.7558,  0.7075]]],\n",
       "       grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_attention = SelfAttention(\n",
    "    d_model = 512,\n",
    "    d_h = 64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "multihead_attention = MultiHeadAttention(\n",
    "    num_heads=8,\n",
    "    d_model=512,\n",
    "    mask=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_block = Encoder(\n",
    "    d_model=512,\n",
    "    num_heads=8,\n",
    "    mask=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_block = Decoder(\n",
    "    d_model=8,\n",
    "    num_heads=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    num_blocks=1,\n",
    "    d_model=512,\n",
    "    num_heads=1,\n",
    "    vocab_size_input=10,\n",
    "    vocab_size_output=10\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
