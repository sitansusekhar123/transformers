{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import torch\n",
    "import json\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_text = \"../data/train.en/train.en\"\n",
    "hindi_text = \"../data/train.hi/train.hi\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize data functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_add_special(text: str) -> list[str]:\n",
    "    tokens = text.split()\n",
    "    tokens = ['sos'] + tokens + ['eos']\n",
    "    return tokens\n",
    "\n",
    "def flatten_and_unique(text_list: list[str]):\n",
    "    tokens_list = [split_and_add_special(t) for t in text_list]\n",
    "    vocabulary = set(token for tokens in tokens_list for token in tokens)\n",
    "    \n",
    "    # Map tokens to integers\n",
    "    vocab_to_index = {token: idx for idx, token in enumerate(vocabulary)}\n",
    "    return vocab_to_index\n",
    "\n",
    "def indexed_tokens(text_list: list[str]):\n",
    "    vocab_map = flatten_and_unique(text_list)\n",
    "    vocab_map = {key: value + 1 for key, value in vocab_map.items()}\n",
    "    \n",
    "    tokens_list = [split_and_add_special(t) for t in text_list]\n",
    "    indexed_tokens = [[vocab_map[token] for token in tokens] for tokens in tokens_list]\n",
    "    return indexed_tokens, vocab_map\n",
    "\n",
    "def indexed_tokens_per_text(text: str, vocab_map):\n",
    "    tokens_list = split_and_add_special(text)\n",
    "    indexed_tokens = [vocab_map[token] for token in tokens_list]\n",
    "    return indexed_tokens\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Train Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(english_text, 'r') as f:\n",
    "    english_data = f.readlines()\n",
    "    \n",
    "with open(hindi_text, 'r', encoding='utf-8') as f:\n",
    "    hindi_data = f.readlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_data = english_data[0:10000]\n",
    "hindi_data = hindi_data[0:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, english_map = indexed_tokens(english_data)\n",
    "_, hindi_map = indexed_tokens(hindi_data)\n",
    "\n",
    "with open('../data/english_map.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(english_map, f, ensure_ascii=False, indent=4)\n",
    "        \n",
    "with open('../data/hindi_map.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(hindi_map, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split(x, y, split_pct: dict):\n",
    "    length = len(x)\n",
    "    \n",
    "    train_index_start = 0\n",
    "    train_index_end = int(split_pct['train'] * length)\n",
    "    x_train, y_train = x[train_index_start: train_index_end], y[train_index_start: train_index_end]\n",
    "    \n",
    "    val_index_start = train_index_end\n",
    "    val_index_end = int(split_pct['validation'] * length + train_index_end)\n",
    "    x_val, y_val = x[val_index_start: val_index_end], y[val_index_start: val_index_end]\n",
    "    \n",
    "    test_index_start = val_index_end\n",
    "    test_index_end = int(split_pct['test'] * length + val_index_end)\n",
    "    x_test, y_test = x[test_index_start: test_index_end], y[test_index_start: test_index_end]\n",
    "    \n",
    "    return {\n",
    "        \"train\":\n",
    "            {\n",
    "                \"x\": x_train,\n",
    "                \"y\": y_train\n",
    "            },\n",
    "        \"test\": \n",
    "            {\n",
    "                \"x\": x_test,\n",
    "                \"y\": y_test\n",
    "            },\n",
    "        \"validation\":\n",
    "            {\n",
    "                \"x\": x_val,\n",
    "                \"y\": y_val\n",
    "            }\n",
    "    }\n",
    "    \n",
    "split_pct = {\n",
    "    \"train\": 0.7,\n",
    "    \"validation\": 0.15,\n",
    "    \"test\": 0.15\n",
    "}\n",
    "data = data_split(english_data, hindi_data, split_pct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in data.items():\n",
    "    with open(f'../data/{key}.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(value, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    x_batch, y_batch = zip(*batch)\n",
    "\n",
    "    # Find maximum sequence length (tokens) in x_batch and y_batch\n",
    "    max_tokens_x = max(len(x) for x in x_batch)\n",
    "    max_tokens_y = max(y.size(0) for y in y_batch)\n",
    "    one_hot_size = y_batch[0].shape[1]  # Dimension of one-hot encoding\n",
    "\n",
    "    # Pad x_batch with zeros (assuming 0 is the padding index for tokens)\n",
    "    x_padded = torch.stack([\n",
    "        torch.nn.functional.pad(\n",
    "            x.clone(),\n",
    "            (0, max_tokens_x - len(x)),\n",
    "            value=0  # Padding index\n",
    "        )\n",
    "        for x in x_batch\n",
    "    ])\n",
    "\n",
    "    # Pad y_batch with zeros along the token dimension\n",
    "    y_padded = torch.stack([\n",
    "        torch.nn.functional.pad(\n",
    "            y,\n",
    "            (0, 0, 0, max_tokens_y - y.shape[0]),  # Padding tokens dimension only\n",
    "            value=0  # Padding with zeros\n",
    "        )\n",
    "        for y in y_batch\n",
    "    ])\n",
    "\n",
    "    return x_padded, y_padded\n",
    "\n",
    "class DatasetLanguage(Dataset):\n",
    "    def __init__(self, data_path: str, x_vocab: str, y_vocab: str):\n",
    "        self.data_path = data_path\n",
    "        with open(data_path, 'r', encoding='utf-8') as f:\n",
    "            self.data = json.load(f)\n",
    "            self.x_data = self.data['x']\n",
    "            self.y_data = self.data['y']\n",
    "            \n",
    "        # load vocab map\n",
    "        with open(x_vocab, 'r', encoding='utf-8') as f:\n",
    "            self.x_vocab = json.load(f)\n",
    "        \n",
    "        with open(y_vocab, 'r', encoding='utf-8') as f:\n",
    "            self.y_vocab = json.load(f)\n",
    "            self.y_output_classes = len(self.y_vocab)\n",
    "            print(self.y_output_classes)\n",
    "                \n",
    "    def get_one_hot_encoding(self, y):\n",
    "        return torch.nn.functional.one_hot(y, num_classes=self.y_output_classes)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x_data = self.x_data[idx]\n",
    "        x_data = indexed_tokens_per_text(x_data, self.x_vocab)\n",
    "        \n",
    "        y_data = self.y_data[idx]\n",
    "        y_data = indexed_tokens_per_text(y_data, self.y_vocab)\n",
    "        y_data_one_hot = self.get_one_hot_encoding(torch.tensor(y_data))\n",
    "        \n",
    "        y_output = {\n",
    "            'y': torch.tensor(y_data),\n",
    "            'y_data_one_hot': y_data_one_hot\n",
    "        }\n",
    "        \n",
    "        return torch.tensor(x_data), y_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27473\n"
     ]
    }
   ],
   "source": [
    "training_data = DatasetLanguage(data_path='../data/validation.json',\n",
    "                                x_vocab='../data/english_map.json',\n",
    "                                y_vocab='../data/hindi_map.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(training_data, batch_size=10, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[196], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_features, train_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\SitansuSekhar\\Documents\\Python_Workspace\\torch_stuff\\transformers\\attention_transformers\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\SitansuSekhar\\Documents\\Python_Workspace\\torch_stuff\\transformers\\attention_transformers\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\SitansuSekhar\\Documents\\Python_Workspace\\torch_stuff\\transformers\\attention_transformers\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[193], line 6\u001b[0m, in \u001b[0;36mcollate_fn\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Find maximum sequence length (tokens) in x_batch and y_batch\u001b[39;00m\n\u001b[0;32m      5\u001b[0m max_tokens_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m x_batch)\n\u001b[1;32m----> 6\u001b[0m max_tokens_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m one_hot_size \u001b[38;5;241m=\u001b[39m y_batch[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# Dimension of one-hot encoding\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Pad x_batch with zeros (assuming 0 is the padding index for tokens)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[193], line 6\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Find maximum sequence length (tokens) in x_batch and y_batch\u001b[39;00m\n\u001b[0;32m      5\u001b[0m max_tokens_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m x_batch)\n\u001b[1;32m----> 6\u001b[0m max_tokens_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m y_batch)\n\u001b[0;32m      7\u001b[0m one_hot_size \u001b[38;5;241m=\u001b[39m y_batch[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# Dimension of one-hot encoding\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Pad x_batch with zeros (assuming 0 is the padding index for tokens)\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'dict' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "train_features, train_labels = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 32])"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 39, 27473])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
