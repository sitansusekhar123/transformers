{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import torch\n",
    "import json\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_text = \"../data/train.en/train.en\"\n",
    "hindi_text = \"../data/train.hi/train.hi\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize data functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_add_special(text: str) -> list[str]:\n",
    "    tokens = text.split()\n",
    "    tokens = ['sos'] + tokens + ['eos']\n",
    "    return tokens\n",
    "\n",
    "def flatten_and_unique(text_list: list[str]):\n",
    "    tokens_list = [split_and_add_special(t) for t in text_list]\n",
    "    vocabulary = set(token for tokens in tokens_list for token in tokens)\n",
    "    \n",
    "    # Map tokens to integers\n",
    "    vocab_to_index = {token: idx for idx, token in enumerate(vocabulary)}\n",
    "    return vocab_to_index\n",
    "\n",
    "def indexed_tokens(text_list: list[str]):\n",
    "    vocab_map = flatten_and_unique(text_list)\n",
    "    vocab_map = {key: value + 1 for key, value in vocab_map.items()}\n",
    "    \n",
    "    tokens_list = [split_and_add_special(t) for t in text_list]\n",
    "    indexed_tokens = [[vocab_map[token] for token in tokens] for tokens in tokens_list]\n",
    "    return indexed_tokens, vocab_map\n",
    "\n",
    "def indexed_tokens_per_text(text: str, vocab_map):\n",
    "    tokens_list = split_and_add_special(text)\n",
    "    indexed_tokens = [vocab_map[token] for token in tokens_list]\n",
    "    return indexed_tokens\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Train Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(english_text, 'r') as f:\n",
    "    english_data = f.readlines()\n",
    "    \n",
    "with open(hindi_text, 'r', encoding='utf-8') as f:\n",
    "    hindi_data = f.readlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_data = english_data[0:10000]\n",
    "hindi_data = hindi_data[0:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, english_map = indexed_tokens(english_data)\n",
    "_, hindi_map = indexed_tokens(hindi_data)\n",
    "\n",
    "with open('../data/english_map.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(english_map, f, ensure_ascii=False, indent=4)\n",
    "        \n",
    "with open('../data/hindi_map.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(hindi_map, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split(x, y, split_pct: dict):\n",
    "    length = len(x)\n",
    "    \n",
    "    train_index_start = 0\n",
    "    train_index_end = int(split_pct['train'] * length)\n",
    "    x_train, y_train = x[train_index_start: train_index_end], y[train_index_start: train_index_end]\n",
    "    \n",
    "    val_index_start = train_index_end\n",
    "    val_index_end = int(split_pct['validation'] * length + train_index_end)\n",
    "    x_val, y_val = x[val_index_start: val_index_end], y[val_index_start: val_index_end]\n",
    "    \n",
    "    test_index_start = val_index_end\n",
    "    test_index_end = int(split_pct['test'] * length + val_index_end)\n",
    "    x_test, y_test = x[test_index_start: test_index_end], y[test_index_start: test_index_end]\n",
    "    \n",
    "    return {\n",
    "        \"train\":\n",
    "            {\n",
    "                \"x\": x_train,\n",
    "                \"y\": y_train\n",
    "            },\n",
    "        \"test\": \n",
    "            {\n",
    "                \"x\": x_test,\n",
    "                \"y\": y_test\n",
    "            },\n",
    "        \"validation\":\n",
    "            {\n",
    "                \"x\": x_val,\n",
    "                \"y\": y_val\n",
    "            }\n",
    "    }\n",
    "    \n",
    "split_pct = {\n",
    "    \"train\": 0.7,\n",
    "    \"validation\": 0.15,\n",
    "    \"test\": 0.15\n",
    "}\n",
    "data = data_split(english_data, hindi_data, split_pct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in data.items():\n",
    "    with open(f'../data/{key}.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(value, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    x_batch, y_batch = zip(*batch)\n",
    "\n",
    "    # Find maximum sequence length (tokens) in x_batch and y_batch\n",
    "    max_tokens_x = max(len(x) for x in x_batch)\n",
    "    max_tokens_y = max(y.size(0) for y in y_batch)\n",
    "    one_hot_size = y_batch[0].shape[1]  # Dimension of one-hot encoding\n",
    "\n",
    "    # Pad x_batch with zeros (assuming 0 is the padding index for tokens)\n",
    "    x_padded = torch.stack([\n",
    "        torch.nn.functional.pad(\n",
    "            x.clone(),\n",
    "            (0, max_tokens_x - len(x)),\n",
    "            value=0  # Padding index\n",
    "        )\n",
    "        for x in x_batch\n",
    "    ])\n",
    "\n",
    "    # Pad y_batch with zeros along the token dimension\n",
    "    y_padded = torch.stack([\n",
    "        torch.nn.functional.pad(\n",
    "            y,\n",
    "            (0, 0, 0, max_tokens_y - y.shape[0]),  # Padding tokens dimension only\n",
    "            value=0  # Padding with zeros\n",
    "        )\n",
    "        for y in y_batch\n",
    "    ])\n",
    "\n",
    "    return x_padded, y_padded\n",
    "\n",
    "class DatasetLanguage(Dataset):\n",
    "    def __init__(self, data_path: str, x_vocab: str, y_vocab: str):\n",
    "        self.data_path = data_path\n",
    "        with open(data_path, 'r', encoding='utf-8') as f:\n",
    "            self.data = json.load(f)\n",
    "            self.x_data = self.data['x']\n",
    "            self.y_data = self.data['y']\n",
    "            \n",
    "        # load vocab map\n",
    "        with open(x_vocab, 'r', encoding='utf-8') as f:\n",
    "            self.x_vocab = json.load(f)\n",
    "        \n",
    "        with open(y_vocab, 'r', encoding='utf-8') as f:\n",
    "            self.y_vocab = json.load(f)\n",
    "            self.y_output_classes = len(self.y_vocab)\n",
    "            print(self.y_output_classes)\n",
    "                \n",
    "    def get_one_hot_encoding(self, y):\n",
    "        return torch.nn.functional.one_hot(y, num_classes=self.y_output_classes)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x_data = self.x_data[idx]\n",
    "        x_data = indexed_tokens_per_text(x_data, self.x_vocab)\n",
    "        \n",
    "        y_data = self.y_data[idx]\n",
    "        y_data = indexed_tokens_per_text(y_data, self.y_vocab)\n",
    "        y_data_one_hot = self.get_one_hot_encoding(torch.tensor(y_data))\n",
    "        \n",
    "        y_output = {\n",
    "            'y': torch.tensor(y_data),\n",
    "            'y_data_one_hot': y_data_one_hot\n",
    "        }\n",
    "        \n",
    "        return torch.tensor(x_data), y_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27473\n"
     ]
    }
   ],
   "source": [
    "training_data = DatasetLanguage(data_path='../data/validation.json',\n",
    "                                x_vocab='../data/english_map.json',\n",
    "                                y_vocab='../data/hindi_map.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(training_data, batch_size=10, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([22, 27473])\n",
      "torch.Size([39, 27473])\n",
      "torch.Size([26, 27473])\n",
      "torch.Size([6, 27473])\n",
      "torch.Size([12, 27473])\n",
      "torch.Size([15, 27473])\n",
      "torch.Size([14, 27473])\n",
      "torch.Size([12, 27473])\n",
      "torch.Size([19, 27473])\n",
      "torch.Size([6, 27473])\n"
     ]
    }
   ],
   "source": [
    "train_features, train_labels = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 32])"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 39, 27473])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
