{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "\n",
    "sys.path.append('../src/')\n",
    "\n",
    "from data_loader import DatasetLanguage, collate_fn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import Transformer\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_path = '../data/train.json'\n",
    "testing_path = '../data/test.json'\n",
    "validation_path = '../data/validation.json'\n",
    "x_vocab = '../data/english_map.json'\n",
    "y_vocab = '../data/hindi_map.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocab Sizes\n",
    "\n",
    "x: 1802939\n",
    "\n",
    "y: 2180936"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: 31018\n",
      "y: 27473\n"
     ]
    }
   ],
   "source": [
    "with open(x_vocab, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "    total_vocab = len(data)\n",
    "    print(f'x: {total_vocab}')\n",
    "    \n",
    "with open(y_vocab, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "    total_vocab = len(data)\n",
    "    print(f'y: {total_vocab}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loders(data_path, batch_size):\n",
    "    dataset = DatasetLanguage(data_path=data_path,\n",
    "                                x_vocab=x_vocab,\n",
    "                                y_vocab=y_vocab)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = get_loders(training_path, batch_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size_input = 31018\n",
    "vocab_size_output = 27473\n",
    "\n",
    "transformer_model = Transformer(\n",
    "    num_blocks=1,\n",
    "    d_model=512,\n",
    "    num_heads=8,\n",
    "    vocab_size_input=vocab_size_input,\n",
    "    vocab_size_output=vocab_size_output\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x0000012507F7BCA0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.001615  [    2/ 7000]\n",
      "loss: 0.001248  [  202/ 7000]\n",
      "loss: 0.001113  [  402/ 7000]\n",
      "loss: 0.002844  [  602/ 7000]\n",
      "loss: 0.001372  [  802/ 7000]\n",
      "loss: 0.000975  [ 1002/ 7000]\n",
      "loss: 0.010503  [ 1202/ 7000]\n",
      "loss: 0.001768  [ 1402/ 7000]\n",
      "loss: 0.001586  [ 1602/ 7000]\n",
      "loss: 0.002822  [ 1802/ 7000]\n",
      "loss: 0.002311  [ 2002/ 7000]\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(transformer_model.parameters(), lr=0.001)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "size = len(train_dataloader.dataset)\n",
    "batch_size = 2\n",
    "epochs = 2\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    transformer_model.train()\n",
    "    for batch, (X, y) in enumerate(train_dataloader):\n",
    "        pred = transformer_model(X, y)\n",
    "        \n",
    "        y_one_hot = torch.nn.functional.one_hot(y, num_classes=vocab_size_output).to(torch.float)\n",
    "        loss = loss_fn(pred, y_one_hot)\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * batch_size + len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
